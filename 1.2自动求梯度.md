在深度学习中，我们经常需要对函数求梯度（gradient）。本节将介绍如何使用PyTorch提供的autograd 模块来自动求梯度。  
```
from torch import autograd
import torch
```
我们求函数y = 2*x^T*x 关于列向量x的梯度。
先创建x并赋值
```
x = torch.arange(4).float()
x
```

Out:  tensor([0., 1., 2., 3.])  
为了求有关变量 的梯度，我们需要将其属性 requires_grad_ 设置为 True ，它将开始追踪
（track）在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。
```
x.requires_grad_(True)
```
接下来构建表达式
```
y = 2*torch.dot(x, x.t()) #.dot()函数： 计算两个张量的点积（内积）
y
```
```
y.backward()#求y对x 的偏导， y=2*x^2,  y`= 4x; 梯度应为4x
```
```
x.grad #输出x的梯度（y对x的偏导数值），不是函数哈
```
Out:   tensor([ 0.,  4.,  8., 12.])  
